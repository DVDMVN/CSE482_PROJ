{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3943, 15)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def load_bunch() -> dict[pd.DataFrame]:\n",
    "    apex = pd.read_csv('processed_data/apex_ad2600_dvd_player_updated.csv')\n",
    "    canon = pd.read_csv('processed_data/canon_g3_updated.csv')\n",
    "    nikon = pd.read_csv('processed_data/nikon_coolpix_4300_updated.csv')\n",
    "    nokia = pd.read_csv('processed_data/nokia_6610_updated.csv')\n",
    "    nomad = pd.read_csv('processed_data/nomad_jukebox_zen_xtra_updated.csv')\n",
    "    return {\n",
    "        \"apex\": apex,\n",
    "        \"canon\": canon,\n",
    "        \"nikon\": nikon,\n",
    "        \"nokia\": nokia,\n",
    "        \"nomad\": nomad\n",
    "    }\n",
    "\n",
    "def get_master_df(sentiments_only: bool = True) -> pd.DataFrame:\n",
    "    bunch = load_bunch()\n",
    "    master_df = pd.concat(bunch.values(), ignore_index=True)\n",
    "    master_df['sentiment_dict'] = master_df['sentiment_dict'].apply(ast.literal_eval)\n",
    "    if sentiments_only:\n",
    "        master_df = master_df[master_df['sentiment_dict'].apply(lambda x: bool(x))]\n",
    "    return master_df\n",
    "\n",
    "master_df = get_master_df(sentiments_only = False)\n",
    "print(master_df.shape)\n",
    "# display(master_df.head(3))\n",
    "\n",
    "\n",
    "# total = 0\n",
    "# bunch: dict = load_bunch()\n",
    "# for name, df in bunch.items():\n",
    "#     df['sentiment_dict'] = df['sentiment_dict'].apply(ast.literal_eval)\n",
    "#     all_sentiment = df[df['sentiment_dict'].apply(lambda x: bool(x))]\n",
    "#     print(\"Total sentences: \", df.shape[0])\n",
    "#     print(\"Sentences with sentiment: \", all_sentiment.shape[0])\n",
    "#     total += all_sentiment.shape[0]\n",
    "# print(total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the sentiment distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_category\n",
       "neutral     2223\n",
       "positive    1082\n",
       "negative     638\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Binning as negative neutral positive\n",
    "# Define conditions\n",
    "conditions = [\n",
    "    master_df['sentiment_total'] > 0,  # Positive sentiment\n",
    "    master_df['sentiment_total'] < 0,  # Negative sentiment\n",
    "    master_df['sentiment_total'] == 0  # Neutral sentiment\n",
    "]\n",
    "\n",
    "# Define corresponding labels\n",
    "labels = ['positive', 'negative', 'neutral']\n",
    "\n",
    "# Create a new column for binned sentiment\n",
    "master_df['sentiment_category'] = np.select(conditions, labels)\n",
    "master_df['sentiment_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "master_df['sentence'] = master_df['sentence'].apply(lambda x: remove_stopwords(str(x)))\n",
    "master_df['tokenized_sentence'] = master_df['sentence'].apply(simple_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train word2vec model: 0.4632248878479004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "\n",
    "train_df, test_df = train_test_split(master_df, test_size=0.2, random_state=1111)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the Word2Vec Model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=train_df['tokenized_sentence'],\n",
    "    vector_size=100,\n",
    "    workers=3,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    sg=1, # Skip Gram\n",
    ")\n",
    "print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating sentence vectors from word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(sentence, model):\n",
    "    valid_words = [word for word in sentence if word in model.wv]\n",
    "    if len(valid_words) == 0:\n",
    "        return np.zeros(model.vector_size)  # Return zero vector if no valid words\n",
    "    return np.mean([model.wv[word] for word in valid_words], axis=0)\n",
    "\n",
    "train_df['sentence_vector'] = train_df['tokenized_sentence'].apply(lambda x: sentence_vector(x, w2v_model))\n",
    "test_df['sentence_vector'] = test_df['tokenized_sentence'].apply(lambda x: sentence_vector(x, w2v_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2.065487884741323, 1: 0.5893124065769806, 2: 1.221060782036392}\n",
      "Training Decision Tree...\n",
      "\n",
      "Decision Tree Accuracy: 0.4639\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.20      0.18      0.19       129\n",
      "     neutral       0.58      0.61      0.59       439\n",
      "    positive       0.35      0.35      0.35       221\n",
      "\n",
      "    accuracy                           0.46       789\n",
      "   macro avg       0.38      0.38      0.38       789\n",
      "weighted avg       0.46      0.46      0.46       789\n",
      "\n",
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Logistic Regression Accuracy: 0.4195\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.25      0.16      0.19       129\n",
      "     neutral       0.63      0.39      0.48       439\n",
      "    positive       0.32      0.63      0.42       221\n",
      "\n",
      "    accuracy                           0.42       789\n",
      "   macro avg       0.40      0.39      0.37       789\n",
      "weighted avg       0.48      0.42      0.42       789\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Random Forest Accuracy: 0.5767\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.02      0.04       129\n",
      "     neutral       0.57      0.97      0.72       439\n",
      "    positive       0.64      0.12      0.21       221\n",
      "\n",
      "    accuracy                           0.58       789\n",
      "   macro avg       0.61      0.37      0.32       789\n",
      "weighted avg       0.60      0.58      0.47       789\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Training and testing data\n",
    "X_train = np.vstack(train_df['sentence_vector'])\n",
    "y_train = train_df['sentiment_category']\n",
    "X_test = np.vstack(test_df['sentence_vector'])\n",
    "y_test = test_df['sentiment_category']\n",
    "\n",
    "# Class weight calculation for balanced models\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', classes=np.unique(y_train), y=y_train\n",
    ")\n",
    "\n",
    "# Using 3 models\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(class_weight='balanced', random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "def train_and_evaluate(models, X_train, y_train, X_test, y_test):\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"\\n{model_name} Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "        print(f\"{model_name} Classification Report:\\n{classification_report(y_test, y_pred)}\\n\")\n",
    "\n",
    "train_and_evaluate(models, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "import random\n",
    "\n",
    "train_df, test_df = train_test_split(master_df, test_size=0.2, random_state=1337)\n",
    "\n",
    "d2v_model = Doc2Vec(\n",
    "    vector_size=100,\n",
    "    workers=3,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "def tag_sentence(tokenized_sentence, tag):\n",
    "    return TaggedDocument(tokenized_sentence, tag)\n",
    "# tag_sentence(utils.simple_preprocess(\"this is a sentence\"), [1])\n",
    "tagged_sentences = []\n",
    "\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "neu_count = 0\n",
    "for _, row in train_df.iterrows():\n",
    "    if row['sentiment_category'] == 'positive':\n",
    "        pos_count += 1\n",
    "        tagged_sentence = tag_sentence(row['tokenized_sentence'], [\"POS_\" + str(pos_count)])\n",
    "    elif row['sentiment_category'] == 'negative':\n",
    "        neg_count += 1\n",
    "        tagged_sentence = tag_sentence(row['tokenized_sentence'], [\"NEG_\" + str(neg_count)])\n",
    "    elif row['sentiment_category'] == 'neutral':\n",
    "        neu_count += 1\n",
    "        tagged_sentence = tag_sentence(row['tokenized_sentence'], [\"NEU_\" + str(neu_count)])\n",
    "    tagged_sentences.append(tagged_sentence)\n",
    "\n",
    "d2v_model.build_vocab(tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_perm(sentences):\n",
    "    shuffled = list(sentences)\n",
    "    random.shuffle(shuffled)\n",
    "    return shuffled\n",
    "\n",
    "for epoch in range(10):\n",
    "    d2v_model.train(sentences_perm(tagged_sentences), total_examples=d2v_model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854\n",
      "514\n",
      "1786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('plastic', 0.7817115187644958),\n",
       " ('uneasy', 0.7758155465126038),\n",
       " ('solid', 0.7733110785484314),\n",
       " ('mechanically', 0.7115648984909058),\n",
       " ('comfy', 0.6967466473579407),\n",
       " ('flimsy', 0.6855295896530151),\n",
       " ('sh', 0.6738451719284058),\n",
       " ('dense', 0.6725516319274902),\n",
       " ('sounding', 0.668586790561676),\n",
       " ('substance', 0.6619100570678711)]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pos_count)\n",
    "print(neg_count)\n",
    "print(neu_count)\n",
    "\n",
    "d2v_model.wv.most_similar(\"feel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = []\n",
    "train_labels = []\n",
    "\n",
    "for i in range(pos_count):\n",
    "    prefix_train = 'POS_' + str(i + 1)\n",
    "    train_array.append(d2v_model[prefix_train])\n",
    "    train_labels.append(1)\n",
    "\n",
    "for i in range(neg_count):\n",
    "    prefix_train = 'NEG_' + str(i + 1)\n",
    "    train_array.append(d2v_model[prefix_train])\n",
    "    train_labels.append(-1)\n",
    "\n",
    "for i in range(neu_count):\n",
    "    prefix_train = 'NEU_' + str(i + 1)\n",
    "    train_array.append(d2v_model[prefix_train])\n",
    "    train_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {-1: 2.0453955901426717, 0: 0.5886524822695035, 1: 1.2310694769711163}\n",
      "Logistic Regression Accuracy: 0.5640050697084917\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.38      0.05      0.09       124\n",
      "           0       0.56      0.98      0.72       437\n",
      "           1       0.91      0.04      0.08       228\n",
      "\n",
      "    accuracy                           0.56       789\n",
      "   macro avg       0.62      0.36      0.29       789\n",
      "weighted avg       0.63      0.56      0.43       789\n",
      "\n",
      "Decision Tree Accuracy: 0.5069708491761724\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.07      0.02      0.04       124\n",
      "           0       0.57      0.80      0.67       437\n",
      "           1       0.34      0.20      0.25       228\n",
      "\n",
      "    accuracy                           0.51       789\n",
      "   macro avg       0.33      0.34      0.32       789\n",
      "weighted avg       0.43      0.51      0.45       789\n",
      "\n",
      "Random Forest Accuracy: 0.5564005069708492\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       124\n",
      "           0       0.56      1.00      0.71       437\n",
      "           1       0.75      0.01      0.03       228\n",
      "\n",
      "    accuracy                           0.56       789\n",
      "   macro avg       0.44      0.34      0.25       789\n",
      "weighted avg       0.52      0.56      0.40       789\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\HW\\Fall_2024_MSU\\CSE482\\bandit\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\HW\\Fall_2024_MSU\\CSE482\\bandit\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\HW\\Fall_2024_MSU\\CSE482\\bandit\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "test_array = []\n",
    "test_labels = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    test_vector = d2v_model.infer_vector(row['tokenized_sentence'])  # Infer vector for test sentence\n",
    "    test_array.append(test_vector)\n",
    "    if row['sentiment_category'] == 'positive':\n",
    "        test_labels.append(1)\n",
    "    elif row['sentiment_category'] == 'negative':\n",
    "        test_labels.append(-1)\n",
    "    elif row['sentiment_category'] == 'neutral':\n",
    "        test_labels.append(0)\n",
    "\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',  # Balances weights inversely proportional to class frequencies\n",
    "    classes=np.unique(train_labels),  # Unique class labels\n",
    "    y=train_labels                # Training labels\n",
    ")\n",
    "\n",
    "# Convert to dictionary format for use in classifiers\n",
    "class_weight_dict = {label: weight for label, weight in zip(np.unique(train_labels), class_weights)}\n",
    "print(\"Class Weights:\", class_weight_dict)\n",
    "\n",
    "# Logistic Regression\n",
    "classifier = LogisticRegression(class_weight='balanced')\n",
    "classifier.fit(train_array, train_labels)\n",
    "y_pred = classifier.predict(test_array)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(test_labels, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(test_labels, y_pred))\n",
    "\n",
    "# Decision Tree\n",
    "classifier = DecisionTreeClassifier(class_weight='balanced')\n",
    "classifier.fit(train_array, train_labels)\n",
    "y_pred = classifier.predict(test_array)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(test_labels, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(test_labels, y_pred))\n",
    "\n",
    "# Random Forest\n",
    "classifier = RandomForestClassifier(class_weight='balanced')\n",
    "classifier.fit(train_array, train_labels)\n",
    "y_pred = classifier.predict(test_array)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(test_labels, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Giving up... GLOVE pretrained try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cherry', 0.9183273911476135)]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-twitter-25\")\n",
    "model.most_similar(positive=['fruit', 'flower'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6070975918884665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.29      0.04      0.07       128\n",
      "     neutral       0.63      0.88      0.73       445\n",
      "    positive       0.55      0.39      0.46       216\n",
      "\n",
      "    accuracy                           0.61       789\n",
      "   macro avg       0.49      0.43      0.42       789\n",
      "weighted avg       0.55      0.61      0.55       789\n",
      "\n",
      "Accuracy:  0.4752851711026616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.20      0.22      0.21       128\n",
      "     neutral       0.61      0.60      0.60       445\n",
      "    positive       0.38      0.38      0.38       216\n",
      "\n",
      "    accuracy                           0.48       789\n",
      "   macro avg       0.40      0.40      0.40       789\n",
      "weighted avg       0.48      0.48      0.48       789\n",
      "\n",
      "Accuracy:  0.6197718631178707\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.21      0.02      0.04       128\n",
      "     neutral       0.63      0.89      0.74       445\n",
      "    positive       0.59      0.41      0.48       216\n",
      "\n",
      "    accuracy                           0.62       789\n",
      "   macro avg       0.48      0.44      0.42       789\n",
      "weighted avg       0.56      0.62      0.56       789\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load GloVe model\n",
    "# model = api.load(\"glove-twitter-25\")\n",
    "\n",
    "# Sample data\n",
    "tokenized_sentences = master_df['tokenized_sentence']  # Your tokenized sentences\n",
    "sentiment_labels = master_df['sentiment_category']     # Corresponding sentiment labels\n",
    "\n",
    "# Generate sentence embeddings\n",
    "def sentence_embedding(sentence_tokens, model):\n",
    "    embeddings = [model[word] for word in sentence_tokens if word in model]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "X = [sentence_embedding(tokens, model) for tokens in tokenized_sentences]\n",
    "y = sentiment_labels\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
