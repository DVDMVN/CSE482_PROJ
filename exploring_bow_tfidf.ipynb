{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_master_df, balance_master_df_classes\n",
    "\n",
    "master_df = load_master_df()\n",
    "master_df = balance_master_df_classes(master_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW (Bag of Words)\n",
    "\n",
    "- Predictor: BoW vector\n",
    "- Target: sentiment_category\n",
    "\n",
    "Steps:\n",
    "1. Train test split the sentences and sentiment_category\n",
    "2. Fit the BoW vectorizer to the training sentences\n",
    "3. Transform both the training sentences and test sentences to BoW vectors using the trained BoW vectorizer\n",
    "4. Test and validate with different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== USING BAG OF WORDS (BoW) ==========\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.8633\n",
      "F1-Score (weighted): 0.8626\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8771    0.9100    0.8933       400\n",
      "     neutral     0.8705    0.7900    0.8283       400\n",
      "    positive     0.8436    0.8900    0.8662       400\n",
      "\n",
      "    accuracy                         0.8633      1200\n",
      "   macro avg     0.8637    0.8633    0.8626      1200\n",
      "weighted avg     0.8637    0.8633    0.8626      1200\n",
      "\n",
      "Feature Importances / Coefficients (Top 10):\n",
      "pleased: 1.4685\n",
      "low: 1.3310\n",
      "suck: 1.3239\n",
      "defective: 1.2859\n",
      "unfortunately: 1.2550\n",
      "junk: 1.2208\n",
      "excellent: 1.2178\n",
      "hell: 1.1913\n",
      "awesome: 1.1778\n",
      "best: 1.1662\n",
      "--------------------------------------------------\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.8717\n",
      "F1-Score (weighted): 0.8706\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8542    0.9375    0.8939       400\n",
      "     neutral     0.8997    0.7850    0.8385       400\n",
      "    positive     0.8665    0.8925    0.8793       400\n",
      "\n",
      "    accuracy                         0.8717      1200\n",
      "   macro avg     0.8735    0.8717    0.8706      1200\n",
      "weighted avg     0.8735    0.8717    0.8706      1200\n",
      "\n",
      "Feature Importances / Coefficients (Top 10):\n",
      "great: 0.0195\n",
      "easy: 0.0164\n",
      "nt: 0.0101\n",
      "buy: 0.0097\n",
      "good: 0.0094\n",
      "pleased: 0.0087\n",
      "excellent: 0.0086\n",
      "feature: 0.0077\n",
      "like: 0.0077\n",
      "lens: 0.0073\n",
      "--------------------------------------------------\n",
      "Model: Random Forest\n",
      "Accuracy: 0.8942\n",
      "F1-Score (weighted): 0.8938\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9128    0.9425    0.9274       400\n",
      "     neutral     0.8805    0.8475    0.8637       400\n",
      "    positive     0.8881    0.8925    0.8903       400\n",
      "\n",
      "    accuracy                         0.8942      1200\n",
      "   macro avg     0.8938    0.8942    0.8938      1200\n",
      "weighted avg     0.8938    0.8942    0.8938      1200\n",
      "\n",
      "Feature Importances / Coefficients (Top 10):\n",
      "great: 0.0134\n",
      "easy: 0.0127\n",
      "player: 0.0095\n",
      "feature: 0.0083\n",
      "nt: 0.0080\n",
      "good: 0.0069\n",
      "pleased: 0.0066\n",
      "work: 0.0065\n",
      "camera: 0.0063\n",
      "buy: 0.0060\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = master_df['sentence']  # or 'tokenized_sentence' if you prefer to feed token lists to a custom vectorizer\n",
    "y = master_df['sentiment_category']\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1111, stratify=y)\n",
    "\n",
    "###############################################################################\n",
    "# 1) Vectorize using Bag-of-Words (BoW)\n",
    "###############################################################################\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=1111),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=1111),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=1111),\n",
    "}\n",
    "\n",
    "print(\"========== USING BAG OF WORDS (BoW) ==========\\n\")\n",
    "for model_name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train_bow, y_train)\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_bow)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')  # Weighted to handle class imbalance\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {f1:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Feature Importances / Coefficients (Top 10):\")\n",
    "    feature_names = np.array(bow_vectorizer.get_feature_names_out())\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # For DecisionTree and RandomForest\n",
    "        importances = model.feature_importances_\n",
    "        # Sort by descending importance\n",
    "        sorted_idx = np.argsort(importances)[::-1]\n",
    "        top_n = 10  # top 10\n",
    "        for idx in sorted_idx[:top_n]:\n",
    "            print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")\n",
    "        \n",
    "    elif hasattr(model, 'coef_'):\n",
    "        # For Logistic Regression\n",
    "        # If multi-class, model.coef_.shape is (n_classes, n_features).\n",
    "        # One way is to average the absolute values across classes:\n",
    "        coefs = model.coef_\n",
    "        importances = np.mean(np.abs(coefs), axis=0)  # shape: (n_features,)\n",
    "        sorted_idx = np.argsort(importances)[::-1]\n",
    "        top_n = 10\n",
    "        for idx in sorted_idx[:top_n]:\n",
    "            print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (text frequency / inverse doc frequency)\n",
    "- Predictor: TF-IDF vector\n",
    "- Target: sentiment_category\n",
    "\n",
    "Steps:\n",
    "1. Train test split the sentences and sentiment_category\n",
    "2. Fit the TF-IDF vectorizer to the training sentences\n",
    "3. Transform both the training sentences and test sentences to TF-IDF vectors using the trained TF-IDF vectorizer\n",
    "4. Test and validate with different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== USING TF-IDF ==========\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.8158\n",
      "F1-Score (weighted): 0.8153\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8469    0.8575    0.8522       400\n",
      "     neutral     0.7916    0.7500    0.7702       400\n",
      "    positive     0.8077    0.8400    0.8235       400\n",
      "\n",
      "    accuracy                         0.8158      1200\n",
      "   macro avg     0.8154    0.8158    0.8153      1200\n",
      "weighted avg     0.8154    0.8158    0.8153      1200\n",
      "\n",
      "Feature Importances / Coefficients (Top 10):\n",
      "easy: 2.2783\n",
      "pleased: 1.8293\n",
      "great: 1.7523\n",
      "best: 1.7181\n",
      "excellent: 1.6575\n",
      "fast: 1.6376\n",
      "low: 1.5074\n",
      "suck: 1.4837\n",
      "love: 1.4656\n",
      "lens: 1.4244\n",
      "--------------------------------------------------\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.8675\n",
      "F1-Score (weighted): 0.8655\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8770    0.9450    0.9097       400\n",
      "     neutral     0.8776    0.7525    0.8102       400\n",
      "    positive     0.8498    0.9050    0.8765       400\n",
      "\n",
      "    accuracy                         0.8675      1200\n",
      "   macro avg     0.8681    0.8675    0.8655      1200\n",
      "weighted avg     0.8681    0.8675    0.8655      1200\n",
      "\n",
      "Feature Importances / Coefficients (Top 10):\n",
      "great: 0.0203\n",
      "player: 0.0202\n",
      "easy: 0.0185\n",
      "buy: 0.0165\n",
      "camera: 0.0127\n",
      "work: 0.0126\n",
      "battery: 0.0110\n",
      "feature: 0.0108\n",
      "phone: 0.0107\n",
      "good: 0.0104\n",
      "--------------------------------------------------\n",
      "Model: Random Forest\n",
      "Accuracy: 0.8900\n",
      "F1-Score (weighted): 0.8894\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9058    0.9375    0.9214       400\n",
      "     neutral     0.8757    0.8275    0.8509       400\n",
      "    positive     0.8873    0.9050    0.8960       400\n",
      "\n",
      "    accuracy                         0.8900      1200\n",
      "   macro avg     0.8896    0.8900    0.8894      1200\n",
      "weighted avg     0.8896    0.8900    0.8894      1200\n",
      "\n",
      "Feature Importances / Coefficients (Top 10):\n",
      "great: 0.0148\n",
      "easy: 0.0128\n",
      "player: 0.0112\n",
      "nt: 0.0096\n",
      "feature: 0.0095\n",
      "work: 0.0084\n",
      "good: 0.0084\n",
      "camera: 0.0082\n",
      "play: 0.0075\n",
      "buy: 0.0074\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(\"========== USING TF-IDF ==========\\n\")\n",
    "for model_name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')  # Weighted to handle class imbalance\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {f1:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    \n",
    "    print(\"Feature Importances / Coefficients (Top 10):\")\n",
    "    feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # For DecisionTree and RandomForest\n",
    "        importances = model.feature_importances_\n",
    "        sorted_idx = np.argsort(importances)[::-1]\n",
    "        top_n = 10\n",
    "        for idx in sorted_idx[:top_n]:\n",
    "            print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")\n",
    "\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        # For Logistic Regression\n",
    "        coefs = model.coef_\n",
    "        importances = np.mean(np.abs(coefs), axis=0)\n",
    "        sorted_idx = np.argsort(importances)[::-1]\n",
    "        top_n = 10\n",
    "        for idx in sorted_idx[:top_n]:\n",
    "            print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")\n",
    "\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
