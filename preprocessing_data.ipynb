{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e1331774-6276-4125-b10b-517a08f55dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "from textblob import TextBlob\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords, words\n",
    "from collections import Counter\n",
    "# from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "094148a6-d694-4a7c-9b61-41f998019724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e64e6-93f2-4983-b37c-2b6cfb159232",
   "metadata": {},
   "source": [
    "### Important Word Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "701a59ed-dfde-4d09-bdb8-caa040e4a064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nt', 'ipod', 'software', 'zen', 'dvd', 'mp3', 'problems', 'features', 'players', '3', 'xtra', 'g3', 'pictures', 'christmas', '2', 'dvds', 'flaws', 'songs', 'using']\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "english_words = set(words.words())\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())  \n",
    "    words = text.split()  \n",
    "    words = [word for word in words if word not in STOPWORDS] \n",
    "    return words\n",
    "\n",
    "def filter_non_standard_words(top_words):\n",
    "    non_standard_words = [word for word, _ in top_words if word not in english_words]\n",
    "    return non_standard_words\n",
    "\n",
    "def word_count_in_folder(folder_path, top_k=100):\n",
    "    all_words = []\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if 'title' in df.columns and 'sentence' in df.columns:\n",
    "                for _, row in df.iterrows():\n",
    "                    text = row['title'] + \" \" + row['sentence']\n",
    "                    words = preprocess_text(text)\n",
    "                    all_words.extend(words)  \n",
    "\n",
    "    word_counts = Counter(all_words)\n",
    "    top_words = word_counts.most_common(top_k)\n",
    "\n",
    "    non_standard_words = filter_non_standard_words(top_words)\n",
    "    \n",
    "    return non_standard_words\n",
    "\n",
    "folder_path = 'data' \n",
    "file_top_words = word_count_in_folder(folder_path, top_k=100) \n",
    "print(file_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9eaa44-daeb-4921-ab7d-e1414af4fe4a",
   "metadata": {},
   "source": [
    "The reason we need to look at the important wordset is because this would let us assess if we should use TextBlob's `correct` function to fix the spellings within the text data. However, we have skipped correcting the spellings because these important words would also get translated and might introduce emotion into the data that did not exist before correction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d88b36-acfb-409f-949a-49a7e77f000a",
   "metadata": {},
   "source": [
    "### Lowercase text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "cede3f96-fa90-443f-bab4-efe51664b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "    \n",
    "    def selective_lowercase(text):\n",
    "        return ' '.join([word if word.isupper() else word.lower() for word in text.split()])\n",
    "    \n",
    "    df['title'] = df['title'].apply(selective_lowercase)\n",
    "    df['sentence'] = df['sentence'].apply(selective_lowercase)\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = contractions.fix(text)\n",
    "        # blob = TextBlob(text)\n",
    "        # expanded_text = str(blob.correct())\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s!?-]', '', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    df['title'] = df['title'].apply(clean_text)\n",
    "    df['sentence'] = df['sentence'].apply(clean_text)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def get_wordnet_pos(nltk_pos):\n",
    "        if nltk_pos.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_pos.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_pos.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_pos.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def conditional_lemmatize(text):\n",
    "        tokens = word_tokenize(text)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        lemmatized_text = []\n",
    "        \n",
    "        for token, pos in pos_tags:\n",
    "            wordnet_pos = get_wordnet_pos(pos)\n",
    "            if wordnet_pos in [wordnet.VERB, wordnet.NOUN]: \n",
    "                lemmatized_text.append(lemmatizer.lemmatize(token, pos=wordnet_pos))\n",
    "            else:\n",
    "                lemmatized_text.append(token)\n",
    "        return ' '.join(lemmatized_text)\n",
    "    \n",
    "    df['title'] = df['title'].apply(conditional_lemmatize)\n",
    "    df['sentence'] = df['sentence'].apply(conditional_lemmatize)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "    # NLTK Tokenizer\n",
    "    #df['title'] = df['title'].apply(word_tokenize)\n",
    "    #df['sentence'] = df['sentence'].apply(word_tokenize)\n",
    "    \n",
    "    def tokenize_text(title, sentence):\n",
    "        title_encoding = tokenizer(title, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "        sentence_encoding = tokenizer(sentence, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "        \n",
    "        return title_encoding['input_ids'].flatten().tolist(), title_encoding['attention_mask'].flatten().tolist(), \\\n",
    "               sentence_encoding['input_ids'].flatten().tolist(), sentence_encoding['attention_mask'].flatten().tolist()\n",
    "    \n",
    "    \n",
    "    df[['title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask']] = df.apply(\n",
    "        lambda row: tokenize_text(row['title'], row['sentence']), axis=1, result_type='expand'\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "3658c501-cc91-4c5a-8cfc-d19dbcaa7d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for _ in os.listdir():\n",
    "    if _.endswith('.csv'):\n",
    "        df = pd.read_csv('data/'+_)\n",
    "        data[_] = data_preprocessing(df)\n",
    "        data[_].to_csv('processed_data/' + _.replace('.csv', '_updated.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
