{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4989f300-8690-4e73-bcfd-31e17c467ff4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Comprehensive Sentiment Analysis and Machine Learning Framework - NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f4bb9-f0a9-410b-b00d-dcad346b4819",
   "metadata": {},
   "source": [
    "\r\n",
    "## Executive Summary\r\n",
    "This advanced Python-based sentiment analysis framework offers a robust, end-to-end solution for processing text data, extracting meaningful insights, and leveraging machine learning techniques. Designed to handle complex text analysis challenges, the system integrates natural language processing (NLP) techniques with machine learning to provide comprehensive sentiment evaluation and predictive modeling.\r\n",
    "\r\n",
    "## Core Capabilities\r\n",
    "The framework combines sophisticated text preprocessing, sentiment analysis, and machine learning to deliver a powerful analytical tool. By leveraging libraries such as NLTK, pandas, and scikit-learn, the system can process multiple CSV files, clean and transform text data, perform nuanced sentiment analysis, and train predictive models.\r\n",
    "\r\n",
    "## Technical Architecture\r\n",
    "\r\n",
    "### Data Processing Pipeline\r\n",
    "- **Automated CSV file discovery and processing**\r\n",
    "- **Intelligent data combination and deduplication**\r\n",
    "- **Advanced null value handling**\r\n",
    "- **Comprehensive text cleaning and preprocessing**\r\n",
    "\r\n",
    "### Sentiment Analysis Methodology\r\n",
    "The sentiment analysis component employs a multi-layered approach:\r\n",
    "\r\n",
    "#### 1. Text Preprocessing\r\n",
    "- Removes URLs, email addresses, and special characters\r\n",
    "- Converts text to lowercase\r\n",
    "- Eliminates stopwords and tokenizes text\r\n",
    "\r\n",
    "#### 2. Sentiment Classification\r\n",
    "Utilizes NLTK's **SentimentIntensityAnalyzer** and provides detailed sentiment categorization:\r\n",
    "- **Very Positive**\r\n",
    "- **Positive**\r\n",
    "- **Slightly Positive**\r\n",
    "- **Neutral**\r\n",
    "- **Slightly Negative**\r\n",
    "- **Negative**\r\n",
    "- **Very Negative**\r\n",
    "\r\n",
    "### Machine Learning Integration\r\n",
    "- **Random Forest Classifier** for predictive modeling\r\n",
    "- Comprehensive **model evaluation metrics**\r\n",
    "- **Feature engineering** and **categorical encoding**\r\n",
    "- Robust **train-test split methodology**\r\n",
    "\r\n",
    "## Key Technical Components\r\n",
    "\r\n",
    "### Libraries and Dependencies\r\n",
    "- **Natural Language Processing:** NLTK\r\n",
    "- **Data Manipulation:** pandas, numpy\r\n",
    "- **Machine Learning:** scikit-learn\r\n",
    "- **System Interaction:** os, glob, datetime\r\n",
    "\r\n",
    "### Primary Functional Modules\r\n",
    "1. **Text Cleaning and Preprocessing**\r\n",
    "2. **Sentiment Score Calculation**\r\n",
    "3. **Machine Learning Model Training**\r\n",
    "4. **Performance Metrics Generation**\r\n",
    "\r\n",
    "## Output and Reporting\r\n",
    "The framework generates comprehensive outputs:\r\n",
    "- Processed data CSV\r\n",
    "- Sentiment distribution analysis\r\n",
    "- Detailed model performance metrics\r\n",
    "- Timestamped output directories\r\n",
    "- Evaluation reports with **accuracy**, **precision**, and **recall** scores\r\n",
    "\r\n",
    "## Practical Applications\r\n",
    "The versatile framework is ideal for:\r\n",
    "- Social media sentiment analysis\r\n",
    "- Customer feedback processing\r\n",
    "- Brand perception monitoring\r\n",
    "- Text classification projects\r\n",
    "- Customer experience insights\r\n",
    "\r\n",
    "## Limitations and Considerations\r\n",
    "While powerful, the framework has some constraints:\r\n",
    "- Primarily designed for **English-language text**\r\n",
    "- **Performance** dependent on input data quality\r\n",
    "- Requires structured **CSV input**\r\n",
    "- Sentiment analysis based on pre-trained lexicons\r\n",
    "\r\n",
    "## Future Development Roadmap\r\n",
    "Potential enhancements include:\r\n",
    "- **Multi-language support**\r\n",
    "- Advanced **deep learning** integration\r\n",
    "- More sophisticated **feature engineering**\r\n",
    "- Customizable sentiment thresholds\r\n",
    "- Enhanced **visualization** capabilities\r\n",
    "\r\n",
    "## Implementation Guidelines\r\n",
    "\r\n",
    "### Prerequisites\r\n",
    "- **Python 3.7+**\r\n",
    "- Required libraries installed\r\n",
    "- Input **CSV files** prepared\r\n",
    "\r\n",
    "### Execution\r\n",
    "1. Place CSV files in the script's directory\r\n",
    "2. Run the script\r\n",
    "3. Review generated output and metrics\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "This sentiment analysis framework represents a sophisticated approach to understanding textual data. By combining advanced NLP techniques with machine learning, it provides organizations with powerful tools to extract meaningful insights from text-based sources.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b80392-d4c4-4ee3-86dd-977076802d33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "908f0211-53b6-4581-a590-ea69f457f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\1520a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\1520a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\1520a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded NLTK resources\n",
      "Starting enhanced sentiment analysis process...\n",
      "\n",
      "Created output directory: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\nltk_sentiment_analysis_outputs\\analysis_20241214_165118\n",
      "\n",
      "Found 5 CSV files:\n",
      "- apex_ad2600_dvd_player_updated.csv\n",
      "- canon_g3_updated.csv\n",
      "- nikon_coolpix_4300_updated.csv\n",
      "- nokia_6610_updated.csv\n",
      "- nomad_jukebox_zen_xtra_updated.csv\n",
      "\n",
      "Processed: apex_ad2600_dvd_player_updated.csv\n",
      "Current shape: (740, 16)\n",
      "\n",
      "Processed: canon_g3_updated.csv\n",
      "Current shape: (1337, 16)\n",
      "\n",
      "Processed: nikon_coolpix_4300_updated.csv\n",
      "Current shape: (1683, 16)\n",
      "\n",
      "Processed: nokia_6610_updated.csv\n",
      "Current shape: (2227, 16)\n",
      "\n",
      "Processed: nomad_jukebox_zen_xtra_updated.csv\n",
      "Current shape: (3943, 16)\n",
      "\n",
      "No obvious text columns found. Available columns are:\n",
      "['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask', 'source_file']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter the name of the text column to analyze:  sentiment_dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing text...\n",
      "\n",
      "Performing sentiment analysis...\n",
      "\n",
      "Results saved to: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\nltk_sentiment_analysis_outputs\\analysis_20241214_165118\\sentiment_analysis_results.csv\n",
      "Summary saved to: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\nltk_sentiment_analysis_outputs\\analysis_20241214_165118\\sentiment_analysis_summary.txt\n",
      "\n",
      "All analysis outputs saved in: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\nltk_sentiment_analysis_outputs\\analysis_20241214_165118\n",
      "\n",
      "Analysis completed successfully!\n",
      "Results are saved in: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\nltk_sentiment_analysis_outputs\\analysis_20241214_165118\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Download required NLTK data (tokenizer, sentiment lexicon, stopwords)\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('vader_lexicon')\n",
    "    nltk.download('stopwords')\n",
    "    print(\"Successfully downloaded NLTK resources\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {str(e)}\")\n",
    "\n",
    "# Function to create an output directory with a timestamp for each run\n",
    "def create_output_directory():\n",
    "    \"\"\"\n",
    "    Create output directory with timestamp\n",
    "    \"\"\"\n",
    "    output_dir = os.path.join(os.path.abspath('.'), 'nltk_sentiment_analysis_outputs')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Create subdirectory with timestamp for current analysis run\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    run_dir = os.path.join(output_dir, f'analysis_{timestamp}')\n",
    "    os.makedirs(run_dir)\n",
    "    \n",
    "    return run_dir\n",
    "\n",
    "# Function to clean the text by removing URLs, emails, special characters, and extra whitespace\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Enhanced text cleaning function\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    # Remove URLs, emails, special characters, numbers, and extra whitespace\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Function to preprocess text (clean it, tokenize, and remove stopwords)\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Enhanced text preprocessing with stopword removal and better tokenization\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stopwords from tokenized text\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to classify sentiment into detailed categories based on compound score and individual scores\n",
    "def get_detailed_sentiment(compound_score, positive_score, negative_score, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Enhanced sentiment classification with more nuanced categories\n",
    "    \"\"\"\n",
    "    # Determine sentiment category based on compound score and positive/negative scores\n",
    "    if compound_score >= threshold:\n",
    "        if positive_score >= 0.5:\n",
    "            return 'very positive'\n",
    "        return 'positive'\n",
    "    elif compound_score <= -threshold:\n",
    "        if negative_score >= 0.5:\n",
    "            return 'very negative'\n",
    "        return 'negative'\n",
    "    elif abs(compound_score) < 0.05:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        if positive_score > negative_score:\n",
    "            return 'slightly positive'\n",
    "        elif negative_score > positive_score:\n",
    "            return 'slightly negative'\n",
    "        return 'neutral'\n",
    "\n",
    "# Main function to process CSV files and perform sentiment analysis\n",
    "def process_csv_and_analyze():\n",
    "    \"\"\"\n",
    "    Enhanced main function with better sentiment analysis and organized output\n",
    "    \"\"\"\n",
    "    # Create output directory and generate subdirectory for the current run\n",
    "    output_dir = create_output_directory()\n",
    "    print(f\"\\nCreated output directory: {output_dir}\")\n",
    "    \n",
    "    # Step 1: Locate and read CSV files in the current directory\n",
    "    directory_path = os.path.abspath('.')\n",
    "    path_pattern = os.path.join(directory_path, '*.csv')\n",
    "    all_files = glob.glob(path_pattern)\n",
    "    \n",
    "    # Write list of processed files to a text file\n",
    "    with open(os.path.join(output_dir, 'processed_files.txt'), 'w') as f:\n",
    "        f.write(\"Processed Files:\\n\")\n",
    "        f.write(\"================\\n\\n\")\n",
    "        for file in all_files:\n",
    "            f.write(f\"- {os.path.basename(file)}\\n\")\n",
    "    \n",
    "    print(f\"\\nFound {len(all_files)} CSV files:\")\n",
    "    for file in all_files:\n",
    "        print(f\"- {os.path.basename(file)}\")\n",
    "    \n",
    "    # Step 2: Read and combine CSV files while removing empty and duplicate rows\n",
    "    combined_df = None\n",
    "    for filename in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(filename)\n",
    "            \n",
    "            # Drop empty rows and duplicate rows\n",
    "            df = df.dropna(how='all').drop_duplicates()\n",
    "            df['source_file'] = os.path.basename(filename)\n",
    "            \n",
    "            if combined_df is None:\n",
    "                combined_df = df\n",
    "            else:\n",
    "                # Concatenate dataframes while avoiding content duplicates\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "                content_columns = [col for col in combined_df.columns \n",
    "                                 if col != 'source_file' and not col.startswith('sentiment_')]\n",
    "                combined_df = combined_df.drop_duplicates(subset=content_columns, keep='first')\n",
    "            \n",
    "            print(f\"\\nProcessed: {os.path.basename(filename)}\")\n",
    "            print(f\"Current shape: {combined_df.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {os.path.basename(filename)}: {str(e)}\")\n",
    "    \n",
    "    if combined_df is None:\n",
    "        raise ValueError(\"No CSV files were successfully read\")\n",
    "    \n",
    "    # Step 3: Identify possible text columns\n",
    "    possible_text_columns = ['text', 'description', 'comment', 'review', 'content', 'message']\n",
    "    text_columns = [col for col in combined_df.columns if any(text_name in col.lower() \n",
    "                                                            for text_name in possible_text_columns)]\n",
    "    \n",
    "    # If no obvious text column is found, prompt the user to specify one\n",
    "    if not text_columns:\n",
    "        print(\"\\nNo obvious text columns found. Available columns are:\")\n",
    "        print(combined_df.columns.tolist())\n",
    "        text_column = input(\"\\nPlease enter the name of the text column to analyze: \")\n",
    "    else:\n",
    "        print(\"\\nFound potential text columns:\", text_columns)\n",
    "        if len(text_columns) == 1:\n",
    "            text_column = text_columns[0]\n",
    "        else:\n",
    "            text_column = input(\"\\nPlease enter the name of the text column to analyze: \")\n",
    "    \n",
    "    # Step 4: Preprocess text by cleaning, tokenizing, and removing stopwords\n",
    "    print(\"\\nPreprocessing text...\")\n",
    "    combined_df['processed_text'] = combined_df[text_column].apply(preprocess_text)\n",
    "    combined_df = combined_df[combined_df['processed_text'].str.len() > 0]  # Remove empty rows\n",
    "    \n",
    "    # Step 5: Perform sentiment analysis using NLTK's SentimentIntensityAnalyzer\n",
    "    print(\"\\nPerforming sentiment analysis...\")\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Get sentiment scores for each processed text\n",
    "    sentiment_scores = combined_df['processed_text'].apply(lambda x: sid.polarity_scores(str(x)))\n",
    "    combined_df['negative'] = sentiment_scores.apply(lambda x: x['neg'])\n",
    "    combined_df['neutral'] = sentiment_scores.apply(lambda x: x['neu'])\n",
    "    combined_df['positive'] = sentiment_scores.apply(lambda x: x['pos'])\n",
    "    combined_df['compound'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "    \n",
    "    # Classify sentiment based on enhanced rules\n",
    "    combined_df['sentiment'] = combined_df.apply(\n",
    "        lambda row: get_detailed_sentiment(\n",
    "            row['compound'],\n",
    "            row['positive'],\n",
    "            row['negative']\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    # Add a confidence score based on the compound score and sentiment strengths\n",
    "    combined_df['sentiment_confidence'] = combined_df.apply(\n",
    "        lambda row: max(abs(row['compound']), max(row['positive'], row['negative'])), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Drop the intermediate 'processed_text' column\n",
    "    final_df = combined_df.drop(['processed_text'], axis=1)\n",
    "    \n",
    "    # Step 6: Generate analysis summary\n",
    "    sentiment_dist = final_df['sentiment'].value_counts()\n",
    "    avg_by_sentiment = final_df.groupby('sentiment')[['negative', 'neutral', 'positive', 'compound']].mean()\n",
    "    \n",
    "    # Step 7: Save the results and summary to output directory\n",
    "    try:\n",
    "        # Verify no duplicates in the final dataset\n",
    "        assert final_df.shape[0] == final_df.drop_duplicates().shape[0], \"Duplicates found in final dataset\"\n",
    "        \n",
    "        # Save results, summary, distribution, and metadata\n",
    "        results_path = os.path.join(output_dir, 'sentiment_analysis_results.csv')\n",
    "        final_df.to_csv(results_path, index=False)\n",
    "        print(f\"\\nResults saved to: {results_path}\")\n",
    "        \n",
    "        summary_path = os.path.join(output_dir, 'sentiment_analysis_summary.txt')\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(\"Enhanced Sentiment Analysis Summary\\n\")\n",
    "            f.write(\"================================\\n\\n\")\n",
    "            f.write(\"Sentiment Distribution:\\n\")\n",
    "            f.write(str(sentiment_dist))\n",
    "            f.write(\"\\n\\nAverage Scores by Sentiment Category:\\n\")\n",
    "            f.write(str(avg_by_sentiment))\n",
    "            f.write(\"\\n\\nConfidence Score Statistics:\\n\")\n",
    "            f.write(str(final_df['sentiment_confidence'].describe()))\n",
    "        print(f\"Summary saved to: {summary_path}\")\n",
    "        \n",
    "        dist_path = os.path.join(output_dir, 'sentiment_distribution.csv')\n",
    "        sentiment_dist.to_frame().to_csv(dist_path)\n",
    "        \n",
    "        meta_path = os.path.join(output_dir, 'analysis_metadata.txt')\n",
    "        with open(meta_path, 'w') as f:\n",
    "            f.write(f\"Analysis Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Total Files Processed: {len(all_files)}\\n\")\n",
    "            f.write(f\"Total Records Analyzed: {len(final_df)}\\n\")\n",
    "            f.write(f\"Text Column Analyzed: {text_column}\\n\")\n",
    "        \n",
    "        print(f\"\\nAll analysis outputs saved in: {output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {str(e)}\")\n",
    "    \n",
    "    return final_df, output_dir\n",
    "\n",
    "# Execute the analysis if the script is run as the main program\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"Starting enhanced sentiment analysis process...\")\n",
    "        results_df, output_path = process_csv_and_analyze()\n",
    "        print(f\"\\nAnalysis completed successfully!\")\n",
    "        print(f\"Results are saved in: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee75403-d11d-43ac-8009-1c82145b1cc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# NLTK - with ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c2300853-826f-47ad-8628-0cc1590ac9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\1520a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\1520a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\1520a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\1520a\\AppData\\Local\\Temp\\ipykernel_53804\\530395791.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\1520a\\AppData\\Local\\Temp\\ipykernel_53804\\530395791.py:104: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded NLTK resources\n",
      "Starting enhanced sentiment analysis and ML process...\n",
      "\n",
      "Created output directory: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\nltk_sentiment_analysis_outputs\\analysis_20241214_165208\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the text column:  sentiment_dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\nltk_sentiment_analysis_outputs\\analysis_20241214_165208\\processed_data.csv\n",
      "\n",
      "Training ML model...\n",
      "Confusion Matrix:\n",
      " [[  0   5   0]\n",
      " [  0 771   0]\n",
      " [  0   9   4]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.98      1.00      0.99       771\n",
      "           2       1.00      0.31      0.47        13\n",
      "\n",
      "    accuracy                           0.98       789\n",
      "   macro avg       0.66      0.44      0.49       789\n",
      "weighted avg       0.98      0.98      0.98       789\n",
      "\n",
      "Accuracy: 0.982256020278834\n",
      "F1 Score (Macro): 0.48719693532940167\n",
      "Precision (Macro): 0.6607218683651804\n",
      "Recall (Macro): 0.4358974358974359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1520a\\anaconda3_new\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\1520a\\anaconda3_new\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\1520a\\anaconda3_new\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\1520a\\anaconda3_new\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    # Download necessary NLTK data files\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('vader_lexicon')\n",
    "    nltk.download('stopwords')\n",
    "    print(\"Successfully downloaded NLTK resources\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {str(e)}\")\n",
    "\n",
    "def create_output_directory():\n",
    "    \"\"\"\n",
    "    Create an output directory with a timestamp for storing results.\n",
    "    \"\"\"\n",
    "    # Define main output directory path\n",
    "    output_dir = os.path.join(os.path.abspath('.'), 'nltk_sentiment_analysis_outputs')\n",
    "    \n",
    "    # If the output directory doesn't exist, create it\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Generate a timestamped subdirectory for this run\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    run_dir = os.path.join(output_dir, f'analysis_{timestamp}')\n",
    "    \n",
    "    # Create the subdirectory\n",
    "    os.makedirs(run_dir)\n",
    "    \n",
    "    return run_dir\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the raw text data by removing unwanted characters, URLs, and emails.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize the text and remove stopwords.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Clean the raw text first\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def handle_null_values(df):\n",
    "    \"\"\"\n",
    "    Replace null values in the dataframe:\n",
    "    - For numeric columns, replace with 0\n",
    "    - For text columns, replace with an empty string\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        # Check if the column is numeric\n",
    "        if df[col].dtype in [np.float64, np.int64]:\n",
    "            df[col].fillna(0, inplace=True)\n",
    "        else:\n",
    "            # For text columns, replace NaN with an empty string\n",
    "            df[col].fillna(\"\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def train_ml_model(df):\n",
    "    \"\"\"\n",
    "    Train a machine learning model (Random Forest) using the sentiment labels and evaluate its performance.\n",
    "    \"\"\"\n",
    "    # Specify the target column for sentiment analysis\n",
    "    target_column = 'sentiment'\n",
    "    \n",
    "    # Check if the target column exists in the dataframe\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"'{target_column}' column not found in the dataset.\")\n",
    "\n",
    "    # Convert categorical sentiment labels to numeric values for model training\n",
    "    df[target_column] = df[target_column].astype('category').cat.codes\n",
    "\n",
    "    # Select feature columns and the target variable\n",
    "    X = df.drop(columns=[target_column, 'source_file'], errors='ignore')  # Drop non-feature columns\n",
    "    y = df[target_column]  # Target: sentiment labels\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "    # Split the data into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a Random Forest model\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # Print additional performance metrics (F1 Score, Precision, Recall)\n",
    "    print(\"F1 Score (Macro):\", f1_score(y_test, y_pred, average='macro'))\n",
    "    print(\"Precision (Macro):\", precision_score(y_test, y_pred, average='macro'))\n",
    "    print(\"Recall (Macro):\", recall_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def process_csv_and_analyze():\n",
    "    \"\"\"\n",
    "    Process CSV files, handle missing values, perform sentiment analysis and train a machine learning model.\n",
    "    \"\"\"\n",
    "    # Create output directory for storing results\n",
    "    output_dir = create_output_directory()\n",
    "    print(f\"\\nCreated output directory: {output_dir}\")\n",
    "\n",
    "    # Step 1: Read all CSV files from the current directory\n",
    "    directory_path = os.path.abspath('.')\n",
    "    path_pattern = os.path.join(directory_path, '*.csv')\n",
    "    all_files = glob.glob(path_pattern)\n",
    "\n",
    "    # Combine all CSV files into one DataFrame\n",
    "    combined_df = pd.concat([pd.read_csv(file).assign(source_file=os.path.basename(file)) for file in all_files], ignore_index=True)\n",
    "\n",
    "    # Step 2: Handle missing values in the DataFrame\n",
    "    combined_df = handle_null_values(combined_df)\n",
    "\n",
    "    # Step 3: Identify possible text columns for sentiment analysis\n",
    "    possible_text_columns = ['text', 'description', 'comment', 'review', 'content', 'message']\n",
    "    text_columns = [col for col in combined_df.columns if any(name in col.lower() for name in possible_text_columns)]\n",
    "    \n",
    "    # Choose the first identified text column (or prompt user if none are found)\n",
    "    text_column = text_columns[0] if text_columns else input(\"Please enter the text column: \")\n",
    "\n",
    "    # Step 4: Preprocess the text data\n",
    "    combined_df['processed_text'] = combined_df[text_column].apply(preprocess_text)\n",
    "\n",
    "    # Step 5: Perform sentiment analysis using VADER sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Get compound sentiment score for each text\n",
    "    combined_df['compound'] = combined_df['processed_text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "    \n",
    "    # Classify sentiment based on compound score\n",
    "    combined_df['sentiment'] = combined_df['compound'].apply(lambda x: 'positive' if x > 0.05 else 'negative' if x < -0.05 else 'neutral')\n",
    "\n",
    "    # Step 6: Save processed data to CSV\n",
    "    results_path = os.path.join(output_dir, 'processed_data.csv')\n",
    "    combined_df.to_csv(results_path, index=False)\n",
    "    print(f\"Processed data saved to: {results_path}\")\n",
    "\n",
    "    # Step 7: Train and evaluate machine learning model\n",
    "    print(\"\\nTraining ML model...\")\n",
    "    train_ml_model(combined_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Main execution flow\n",
    "    print(\"Starting enhanced sentiment analysis and ML process...\")\n",
    "    process_csv_and_analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7443e6-77e4-484a-af46-c6ad206cc3e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sentiment Analysis and Text Processing Code Overview - BOW Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4722d5c-62a8-4ac4-8134-b37d7181d06e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The provided Python code performs a comprehensive text analysis on CSV files by applying the Bag of Words (BoW) methodology. It includes steps for preprocessing text data, extracting word and bigram frequencies, calculating TF-IDF (Term Frequency-Inverse Document Frequency) scores, generating visualizations (e.g., word clouds), and producing a summary report. The code integrates several libraries and processes text data to generate insightful metrics and visual outputs.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "### 1. **Package Installation**\n",
    "The script checks for required packages and installs them if necessary. This includes:\n",
    "- **pandas** for data manipulation\n",
    "- **numpy** for numerical operations\n",
    "- **scikit-learn** for machine learning and text vectorization\n",
    "- **nltk** for natural language processing\n",
    "- **wordcloud** for visualizing text data\n",
    "- **matplotlib** and **seaborn** for creating visualizations\n",
    "\n",
    "The `install_requirements` function ensures that the required libraries are installed by checking if they are already available and installing them if they are missing.\n",
    "\n",
    "### 2. **Text Preprocessing**\n",
    "The `clean_text` function is responsible for cleaning and preparing text for analysis:\n",
    "- Converts text to lowercase\n",
    "- Removes URLs, email addresses, special characters, and numbers\n",
    "- Eliminates extra whitespaces\n",
    "\n",
    "This function is applied to the specified text column in the dataset to prepare the text for further analysis.\n",
    "\n",
    "### 3. **Directory Management**\n",
    "The `create_output_directory` function creates a new directory with a timestamp where all analysis results will be saved. This helps organize output files for each run.\n",
    "\n",
    "### 4. **Bag of Words (BoW) and TF-IDF Analysis**\n",
    "The `perform_bow_analysis` function conducts the core of the analysis:\n",
    "- **BoW**: This method calculates the frequency of words that appear in the dataset.\n",
    "- **TF-IDF**: This method calculates the importance of words based on their frequency relative to all documents in the dataset.\n",
    "- **Bigrams**: It also calculates the frequency of two-word combinations (bigrams) using the CountVectorizer.\n",
    "\n",
    "Results from these analyses are saved in CSV files for further use. The function also generates visualizations such as a word cloud to represent the most frequent words.\n",
    "\n",
    "### 5. **Visualization**\n",
    "- **Word Cloud**: The script uses the `wordcloud` library to generate a word cloud visualizing the frequency of terms found in the text. This is saved as an image file.\n",
    "- **Bigram Analysis**: Common bigrams (two-word phrases) are calculated, and the results are saved in a CSV file.\n",
    "\n",
    "### 6. **Summary Report**\n",
    "A text file summary (`text_analysis_summary.txt`) is created, which includes:\n",
    "- Document statistics (e.g., total documents, average document length)\n",
    "- Top 20 most frequent words\n",
    "- Top 20 most important words based on TF-IDF scores\n",
    "- Top 20 most common bigrams\n",
    "\n",
    "### 7. **File Management**\n",
    "The script reads all CSV files in the current directory, combines them into a single DataFrame, and processes the text column. The user is prompted to select the correct text column if it is not automatically detected.\n",
    "\n",
    "### 8. **Error Handling**\n",
    "Error handling is implemented throughout the code:\n",
    "- If required packages cannot be installed, an error is raised.\n",
    "- If there are issues reading the CSV files, the script will notify the user.\n",
    "- Text analysis errors are captured, and detailed error messages are provided.\n",
    "\n",
    "## Output Files\n",
    "Upon successful execution, the script generates the following output files:\n",
    "1. **word_analysis.csv**: Contains word frequencies and corresponding TF-IDF scores.\n",
    "2. **bigram_analysis.csv**: Contains the frequency of bigrams (two-word combinations).\n",
    "3. **wordcloud.png**: A visual word cloud generated from the most frequent words.\n",
    "4. **text_analysis_summary.txt**: A detailed text analysis report.\n",
    "5. **processed_data.csv**: A cleaned dataset with the text data preprocessed.\n",
    "\n",
    "## Workflow\n",
    "1. **Install required packages**: Ensures necessary libraries are installed.\n",
    "2. **Read CSV files**: All CSV files in the current directory are read and combined into a single DataFrame.\n",
    "3. **Text preprocessing**: The text data is cleaned (removing URLs, emails, special characters, etc.).\n",
    "4. **BoW and TF-IDF Analysis**: Word frequencies and TF-IDF scores are calculated.\n",
    "5. **Bigram Analysis**: Common two-word combinations are identified.\n",
    "6. **Generate output**: Results are saved in various files, and visualizations are created.\n",
    "\n",
    "## Conclusion\n",
    "This Python code provides a comprehensive framework for text analysis, specifically using the Bag of Words and TF-IDF methods. It offers a powerful tool for processing text data, generating insights into word frequency, importance, and common bigrams. Additionally, the script produces visual and textual reports that summarize the findings, making it suitable for various text analysis applications.\n",
    "\n",
    "By following the outlined structure, users can perform detailed text analysis on their datasets and extract meaningful insights for further decision-making or reporting purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0faad92-bbfd-48ce-aed5-ae01e1f0a3a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "34e8b28a-ed21-4247-86c1-e8a53b2adfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and installing required packages...\n",
      "✓ pandas already installed\n",
      "✓ numpy already installed\n",
      "Installing scikit-learn...\n",
      "✓ scikit-learn installed successfully\n",
      "✓ nltk already installed\n",
      "✓ wordcloud already installed\n",
      "✓ matplotlib already installed\n",
      "✓ seaborn already installed\n",
      "✓ Successfully downloaded NLTK resources\n",
      "\n",
      "Created output directory: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\bow_text_analysis_outputs\\bow_analysis_20241214_165513\n",
      "Read file: apex_ad2600_dvd_player_updated.csv\n",
      "Read file: canon_g3_updated.csv\n",
      "Read file: nikon_coolpix_4300_updated.csv\n",
      "Read file: nokia_6610_updated.csv\n",
      "Read file: nomad_jukebox_zen_xtra_updated.csv\n",
      "\n",
      "No obvious text columns found. Available columns are:\n",
      "['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter the name of the text column to analyze:  sentence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Bag of Words analysis...\n",
      "Cleaning text data...\n",
      "Performing text vectorization...\n",
      "Saving analysis results...\n",
      "Generating word cloud...\n",
      "Analyzing bigrams...\n",
      "Generating summary report...\n",
      "\n",
      "Analysis completed! Results saved in: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\bow_text_analysis_outputs\\bow_analysis_20241214_165513\n",
      "\n",
      "Files generated:\n",
      "1. word_analysis.csv - Word frequencies and TF-IDF scores\n",
      "2. bigram_analysis.csv - Common word pairs analysis\n",
      "3. wordcloud.png - Visual representation of word frequencies\n",
      "4. text_analysis_summary.txt - Detailed analysis report\n",
      "5. processed_data.csv - Processed dataset with cleaned text\n",
      "\n",
      "Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"\n",
    "    Install required packages if they're not already installed\n",
    "    \"\"\"\n",
    "    required_packages = [\n",
    "        'pandas',\n",
    "        'numpy',\n",
    "        'scikit-learn',\n",
    "        'nltk',\n",
    "        'wordcloud',\n",
    "        'matplotlib',\n",
    "        'seaborn'\n",
    "    ]\n",
    "    \n",
    "    print(\"Checking and installing required packages...\")\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"✓ {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"✓ {package} installed successfully\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the analysis\n",
    "    \"\"\"\n",
    "    # First install requirements\n",
    "    install_requirements()\n",
    "    \n",
    "    # Now import required packages\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    from collections import Counter\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import matplotlib.pyplot as plt\n",
    "    from wordcloud import WordCloud\n",
    "    import seaborn as sns\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    import re\n",
    "    import glob\n",
    "    \n",
    "    # Download required NLTK data\n",
    "    try:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        print(\"✓ Successfully downloaded NLTK resources\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading NLTK resources: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    def create_output_directory():\n",
    "        \"\"\"\n",
    "        Create output directory with timestamp\n",
    "        \"\"\"\n",
    "        output_dir = os.path.join(os.path.abspath('.'), 'bow_text_analysis_outputs')\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        run_dir = os.path.join(output_dir, f'bow_analysis_{timestamp}')\n",
    "        os.makedirs(run_dir)\n",
    "        \n",
    "        return run_dir\n",
    "\n",
    "    def clean_text(text):\n",
    "        \"\"\"\n",
    "        Clean text for analysis\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string and lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def perform_bow_analysis(df, text_column, output_dir):\n",
    "        \"\"\"\n",
    "        Perform Bag of Words analysis\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting Bag of Words analysis...\")\n",
    "        \n",
    "        # Verify text column exists\n",
    "        if text_column not in df.columns:\n",
    "            raise ValueError(f\"Column '{text_column}' not found in dataset. Available columns: {', '.join(df.columns)}\")\n",
    "        \n",
    "        # Clean the text\n",
    "        print(\"Cleaning text data...\")\n",
    "        df['cleaned_text'] = df[text_column].apply(clean_text)\n",
    "        \n",
    "        # Remove empty texts\n",
    "        df = df[df['cleaned_text'].str.len() > 0].reset_index(drop=True)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(\"No valid text data remaining after cleaning\")\n",
    "        \n",
    "        # Initialize vectorizers\n",
    "        print(\"Performing text vectorization...\")\n",
    "        # Convert stop words to list instead of set\n",
    "        stop_words = list(stopwords.words('english'))\n",
    "        \n",
    "        # CountVectorizer for basic word frequency\n",
    "        count_vec = CountVectorizer(max_features=1000, \n",
    "                                  stop_words=stop_words,  # Now using list instead of set\n",
    "                                  min_df=2)\n",
    "        \n",
    "        # TF-IDF Vectorizer for word importance\n",
    "        tfidf_vec = TfidfVectorizer(max_features=1000,\n",
    "                                   stop_words=stop_words,  # Now using list instead of set\n",
    "                                   min_df=2)\n",
    "        \n",
    "        try:\n",
    "            # Fit and transform the text\n",
    "            bow_matrix = count_vec.fit_transform(df['cleaned_text'])\n",
    "            tfidf_matrix = tfidf_vec.fit_transform(df['cleaned_text'])\n",
    "            \n",
    "            # Get feature names\n",
    "            feature_names = count_vec.get_feature_names_out()\n",
    "            \n",
    "            # Calculate word frequencies\n",
    "            word_freq = pd.DataFrame(bow_matrix.sum(axis=0).T,\n",
    "                                   index=feature_names,\n",
    "                                   columns=['frequency']).sort_values('frequency', ascending=False)\n",
    "            \n",
    "            # Calculate TF-IDF scores\n",
    "            tfidf_scores = pd.DataFrame(tfidf_matrix.mean(axis=0).T,\n",
    "                                      index=feature_names,\n",
    "                                      columns=['tfidf_score']).sort_values('tfidf_score', ascending=False)\n",
    "            \n",
    "            # Combine frequencies and TF-IDF scores\n",
    "            word_analysis = pd.merge(word_freq, tfidf_scores,\n",
    "                                   left_index=True, right_index=True,\n",
    "                                   how='outer').fillna(0)\n",
    "            \n",
    "            # Save results\n",
    "            print(\"Saving analysis results...\")\n",
    "            word_analysis.to_csv(os.path.join(output_dir, 'word_analysis.csv'))\n",
    "            \n",
    "            # Generate and save word clouds\n",
    "            print(\"Generating word cloud...\")\n",
    "            plt.figure(figsize=(20,10))\n",
    "            try:\n",
    "                wordcloud = WordCloud(width=1600, height=800,\n",
    "                                    background_color='white',\n",
    "                                    max_words=100).generate_from_frequencies(\n",
    "                                        dict(zip(word_freq.index, word_freq['frequency']))\n",
    "                                    )\n",
    "                \n",
    "                plt.imshow(wordcloud, interpolation='bilinear')\n",
    "                plt.axis('off')\n",
    "                plt.title('Word Cloud of Most Frequent Terms')\n",
    "                plt.savefig(os.path.join(output_dir, 'wordcloud.png'), bbox_inches='tight', dpi=300)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not generate word cloud: {str(e)}\")\n",
    "            finally:\n",
    "                plt.close()\n",
    "            \n",
    "            # Calculate bigrams\n",
    "            print(\"Analyzing bigrams...\")\n",
    "            bigram_vectorizer = CountVectorizer(ngram_range=(2,2),\n",
    "                                              max_features=100,\n",
    "                                              stop_words=stop_words)  # Now using list instead of set\n",
    "            bigram_matrix = bigram_vectorizer.fit_transform(df['cleaned_text'])\n",
    "            bigram_freq = pd.DataFrame(bigram_matrix.sum(axis=0).T,\n",
    "                                      index=bigram_vectorizer.get_feature_names_out(),\n",
    "                                      columns=['frequency']).sort_values('frequency', ascending=False)\n",
    "            \n",
    "            bigram_freq.to_csv(os.path.join(output_dir, 'bigram_analysis.csv'))\n",
    "            \n",
    "            # Generate summary report\n",
    "            print(\"Generating summary report...\")\n",
    "            with open(os.path.join(output_dir, 'text_analysis_summary.txt'), 'w') as f:\n",
    "                f.write(\"Text Analysis Summary\\n\")\n",
    "                f.write(\"===================\\n\\n\")\n",
    "                \n",
    "                f.write(\"Document Statistics:\\n\")\n",
    "                f.write(f\"Total documents analyzed: {len(df)}\\n\")\n",
    "                f.write(f\"Average document length: {df['cleaned_text'].str.len().mean():.1f} characters\\n\")\n",
    "                f.write(f\"Unique words analyzed: {len(feature_names)}\\n\\n\")\n",
    "                \n",
    "                f.write(\"Top 20 Most Frequent Words:\\n\")\n",
    "                f.write(str(word_freq.head(20)))\n",
    "                f.write(\"\\n\\nTop 20 Most Important Words (TF-IDF):\\n\")\n",
    "                f.write(str(tfidf_scores.head(20)))\n",
    "                f.write(\"\\n\\nTop 20 Most Common Bigrams:\\n\")\n",
    "                f.write(str(bigram_freq.head(20)))\n",
    "            \n",
    "            # Save processed dataset\n",
    "            df.to_csv(os.path.join(output_dir, 'processed_data.csv'), index=False)\n",
    "            \n",
    "            return word_analysis, bigram_freq, df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during text analysis: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = create_output_directory()\n",
    "    print(f\"\\nCreated output directory: {output_dir}\")\n",
    "    \n",
    "    # Read all CSV files in directory\n",
    "    csv_files = glob.glob('*.csv')\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found in current directory!\")\n",
    "        return\n",
    "    \n",
    "    # Read and combine all CSV files\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            print(f\"Read file: {file}\")\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {str(e)}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        print(\"No valid CSV files could be read!\")\n",
    "        return\n",
    "        \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Identify text column\n",
    "    possible_text_columns = ['text', 'description', 'comment', 'review', 'content', 'message']\n",
    "    text_columns = [col for col in combined_df.columns \n",
    "                   if any(text_name in col.lower() for text_name in possible_text_columns)]\n",
    "    \n",
    "    if not text_columns:\n",
    "        print(\"\\nNo obvious text columns found. Available columns are:\")\n",
    "        print(combined_df.columns.tolist())\n",
    "        text_column = input(\"\\nPlease enter the name of the text column to analyze: \")\n",
    "    else:\n",
    "        print(\"\\nFound potential text columns:\", text_columns)\n",
    "        if len(text_columns) == 1:\n",
    "            text_column = text_columns[0]\n",
    "        else:\n",
    "            text_column = input(\"\\nPlease enter the name of the text column to analyze: \")\n",
    "    \n",
    "    try:\n",
    "        # Perform analysis\n",
    "        word_analysis, bigram_freq, processed_df = perform_bow_analysis(combined_df, text_column, output_dir)\n",
    "        \n",
    "        print(f\"\\nAnalysis completed! Results saved in: {output_dir}\")\n",
    "        print(\"\\nFiles generated:\")\n",
    "        print(\"1. word_analysis.csv - Word frequencies and TF-IDF scores\")\n",
    "        print(\"2. bigram_analysis.csv - Common word pairs analysis\")\n",
    "        print(\"3. wordcloud.png - Visual representation of word frequencies\")\n",
    "        print(\"4. text_analysis_summary.txt - Detailed analysis report\")\n",
    "        print(\"5. processed_data.csv - Processed dataset with cleaned text\")\n",
    "        \n",
    "        return word_analysis, bigram_freq, processed_df, output_dir\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during analysis: {str(e)}\")\n",
    "        return None, None, None, output_dir\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        results = main()\n",
    "        if results[0] is not None:\n",
    "            print(f\"\\nAnalysis completed successfully!\")\n",
    "        else:\n",
    "            print(\"\\nAnalysis completed with errors. Please check the output directory for partial results.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296407fc-e960-4116-aea4-a7c61c5a868c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sentiment Analysis and Text Processing Code Overview - TFIDF Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e135e-fc7f-4f70-bfbf-e20b992fc702",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This script is designed to perform **TF-IDF (Term Frequency-Inverse Document Frequency)** analysis on text data contained within CSV files. It handles the installation of necessary libraries, prepares the data, performs the TF-IDF analysis, and generates various output files and visualizations to summarize the results.\n",
    "\n",
    "## Key Steps in the Process\n",
    "\n",
    "### 1. **Install Required Libraries**\n",
    "The script first ensures that the required Python libraries are installed, including:\n",
    "- `pandas`: For data manipulation and analysis.\n",
    "- `numpy`: For numerical computations.\n",
    "- `scikit-learn`: For machine learning and vectorization tools (TF-IDF).\n",
    "- `nltk`: For natural language processing tasks, including stopwords and tokenization.\n",
    "- `matplotlib` and `seaborn`: For visualizations.\n",
    "\n",
    "If any required library is not installed, it attempts to install it using `pip`.\n",
    "\n",
    "### 2. **Data Preprocessing**\n",
    "The script processes the text data in the following steps:\n",
    "- **Cleaning Text**: It removes URLs, email addresses, special characters, and numbers. It also converts the text to lowercase and eliminates extra whitespaces.\n",
    "- **Stopwords Removal**: It uses the list of English stopwords from the `nltk` library to filter out common words like \"the\", \"and\", etc.\n",
    "  \n",
    "### 3. **TF-IDF Analysis**\n",
    "The TF-IDF analysis is performed using the `TfidfVectorizer` from the `scikit-learn` library. The key steps in this analysis include:\n",
    "- **Vectorization**: The text data is converted into a numerical matrix using both **unigrams** and **bigrams** (n-grams of length 1 and 2). The vectorizer is configured to ignore terms that appear in fewer than 2 documents or more than 95% of documents.\n",
    "- **TF-IDF Calculation**: The script calculates the importance of each term in the context of the documents using the TF-IDF formula:\n",
    "    - **Term Frequency (TF)**: Measures how often a term appears in a document.\n",
    "    - **Inverse Document Frequency (IDF)**: Measures how important the term is across all documents. Rare terms have higher IDF scores.\n",
    "    \n",
    "### 4. **Output Directory and Files**\n",
    "The script creates an output directory with a timestamp to store the results, including:\n",
    "- **`average_tfidf_scores.csv`**: Contains the average TF-IDF score for each term.\n",
    "- **`document_term_matrix.csv`**: A matrix of terms versus documents showing the TF-IDF scores for each term in each document.\n",
    "- **Visualizations**:\n",
    "    - **Top 20 Terms by TF-IDF**: A bar plot showing the terms with the highest average TF-IDF scores.\n",
    "    - **TF-IDF Distribution**: A histogram showing the distribution of TF-IDF scores across all terms.\n",
    "    - **Document Similarity Matrix**: A heatmap showing the similarity between documents based on their TF-IDF vectors.\n",
    "- **`tfidf_analysis_summary.txt`**: A text summary of the analysis, including statistics about the documents and terms, as well as the top terms and most unique terms based on their TF-IDF scores.\n",
    "  \n",
    "### 5. **Error Handling**\n",
    "The script includes error handling for common issues, such as missing text columns, invalid CSV files, and errors during the installation of required packages or during the TF-IDF analysis itself.\n",
    "\n",
    "## Main Functions\n",
    "\n",
    "### `install_requirements()`\n",
    "This function checks if the required libraries (`pandas`, `numpy`, `scikit-learn`, `nltk`, `matplotlib`, `seaborn`) are installed. If any package is missing, it attempts to install it.\n",
    "\n",
    "### `create_output_directory()`\n",
    "Creates a directory for storing the analysis results with a timestamp to distinguish between different runs.\n",
    "\n",
    "### `clean_text(text)`\n",
    "Cleans the input text by:\n",
    "- Converting it to lowercase.\n",
    "- Removing URLs, email addresses, special characters, and digits.\n",
    "- Removing extra whitespace.\n",
    "\n",
    "### `perform_tfidf_analysis(df, text_column, output_dir)`\n",
    "This function performs the actual TF-IDF analysis:\n",
    "- **Cleaning the text**: Cleans the text data in the specified column of the DataFrame.\n",
    "- **Vectorization**: Applies the `TfidfVectorizer` to convert the cleaned text into a matrix of TF-IDF scores.\n",
    "- **Matrix Creation**: Creates a document-term matrix with the TF-IDF scores for each term in each document.\n",
    "- **Visualizations**: Generates and saves plots for the top terms, the distribution of TF-IDF scores, and document similarity.\n",
    "- **Summary Report**: Saves a text file with a summary of the analysis, including statistics about the documents, terms, and most unique terms based on TF-IDF scores.\n",
    "\n",
    "### `main()`\n",
    "The main function of the script:\n",
    "1. Installs required libraries.\n",
    "2. Reads and processes CSV files.\n",
    "3. Identifies a text column for analysis (based on common column names like 'text', 'description', etc.).\n",
    "4. Calls the `perform_tfidf_analysis()` function to run the TF-IDF analysis.\n",
    "5. Saves the results in the output directory.\n",
    "\n",
    "### Error Handling in `main()`\n",
    "- The script checks for the existence of text columns and requests user input if no suitable column is automatically found.\n",
    "- Errors during the analysis or CSV reading are caught and displayed to the user.\n",
    "\n",
    "## Conclusion\n",
    "The script provides a comprehensive TF-IDF analysis of textual data, with multiple output files and visualizations to summarize the results. It is designed to handle various types of text columns and ensures that required libraries are installed and available.\n",
    "\n",
    "## Output Files\n",
    "Upon successful execution, the following files are generated in the output directory:\n",
    "1. **`average_tfidf_scores.csv`** - Contains the average TF-IDF scores for each term.\n",
    "2. **`document_term_matrix.csv`** - Full document-term matrix.\n",
    "3. **`top_terms_tfidf.png`** - Bar plot of the top 20 terms by average TF-IDF score.\n",
    "4. **`tfidf_distribution.png`** - Histogram of TF-IDF score distribution.\n",
    "5. **`document_similarity.png`** - Heatmap of document similarity.\n",
    "6. **`tfidf_analysis_summary.txt`** - Text summary of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6a14d-43bb-4fcf-8457-2bd2ce081b04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "70f4cb99-da23-4819-88e1-ae90c446585d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and installing required packages...\n",
      "✓ pandas already installed\n",
      "✓ numpy already installed\n",
      "Installing scikit-learn...\n",
      "✓ scikit-learn installed successfully\n",
      "✓ nltk already installed\n",
      "✓ matplotlib already installed\n",
      "✓ seaborn already installed\n",
      "✓ Successfully downloaded NLTK resources\n",
      "\n",
      "Created output directory: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\tfidf_analysis_outputs\\tfidf_analysis_20241125_173018\n",
      "Read file: apex_ad2600_dvd_player_updated.csv\n",
      "Read file: canon_g3_updated.csv\n",
      "Read file: nikon_coolpix_4300_updated.csv\n",
      "Read file: nokia_6610_updated.csv\n",
      "Read file: nomad_jukebox_zen_xtra_updated.csv\n",
      "\n",
      "No obvious text columns found. Available columns are:\n",
      "['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter the name of the text column to analyze:  sentence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting TF-IDF analysis...\n",
      "Cleaning text data...\n",
      "Performing TF-IDF vectorization...\n",
      "Saving analysis results...\n",
      "Generating visualizations...\n",
      "Calculating document similarity...\n",
      "Generating summary report...\n",
      "\n",
      "Analysis completed! Results saved in: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\tfidf_analysis_outputs\\tfidf_analysis_20241125_173018\n",
      "\n",
      "Files generated:\n",
      "1. average_tfidf_scores.csv - Average TF-IDF scores for each term\n",
      "2. document_term_matrix.csv - Full document-term matrix\n",
      "3. top_terms_tfidf.png - Bar plot of top terms by TF-IDF score\n",
      "4. tfidf_distribution.png - Distribution of TF-IDF scores\n",
      "5. document_similarity.png - Heatmap of document similarity\n",
      "6. tfidf_analysis_summary.txt - Detailed analysis report\n",
      "\n",
      "Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"\n",
    "    Install required packages if they're not already installed\n",
    "    \"\"\"\n",
    "    required_packages = [\n",
    "        'pandas',\n",
    "        'numpy',\n",
    "        'scikit-learn',\n",
    "        'nltk',\n",
    "        'matplotlib',\n",
    "        'seaborn'\n",
    "    ]\n",
    "    \n",
    "    print(\"Checking and installing required packages...\")\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"✓ {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"✓ {package} installed successfully\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the TF-IDF analysis\n",
    "    \"\"\"\n",
    "    # First install requirements\n",
    "    install_requirements()\n",
    "    \n",
    "    # Now import required packages\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    import re\n",
    "    import glob\n",
    "    \n",
    "    # Download required NLTK data\n",
    "    try:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        print(\"✓ Successfully downloaded NLTK resources\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading NLTK resources: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    def create_output_directory():\n",
    "        \"\"\"Create output directory with timestamp\"\"\"\n",
    "        output_dir = os.path.join(os.path.abspath('.'), 'tfidf_analysis_outputs')\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        run_dir = os.path.join(output_dir, f'tfidf_analysis_{timestamp}')\n",
    "        os.makedirs(run_dir)\n",
    "        \n",
    "        return run_dir\n",
    "\n",
    "    def clean_text(text):\n",
    "        \"\"\"Clean text for analysis\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string and lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def perform_tfidf_analysis(df, text_column, output_dir):\n",
    "        \"\"\"Perform TF-IDF analysis\"\"\"\n",
    "        print(\"\\nStarting TF-IDF analysis...\")\n",
    "        \n",
    "        # Verify text column exists\n",
    "        if text_column not in df.columns:\n",
    "            raise ValueError(f\"Column '{text_column}' not found in dataset. Available columns: {', '.join(df.columns)}\")\n",
    "        \n",
    "        # Clean the text\n",
    "        print(\"Cleaning text data...\")\n",
    "        df['cleaned_text'] = df[text_column].apply(clean_text)\n",
    "        \n",
    "        # Remove empty texts\n",
    "        df = df[df['cleaned_text'].str.len() > 0].reset_index(drop=True)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(\"No valid text data remaining after cleaning\")\n",
    "        \n",
    "        # Initialize vectorizer\n",
    "        print(\"Performing TF-IDF vectorization...\")\n",
    "        stop_words = list(stopwords.words('english'))\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            stop_words=stop_words,\n",
    "            min_df=2,         # Ignore terms that appear in less than 2 documents\n",
    "            max_df=0.95,      # Ignore terms that appear in more than 95% of documents\n",
    "            ngram_range=(1,2) # Include both unigrams and bigrams\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Fit and transform the text\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "            feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Convert to dense array for analysis\n",
    "            dense_tfidf = tfidf_matrix.todense()\n",
    "            \n",
    "            # Calculate document-term importance\n",
    "            doc_term_matrix = pd.DataFrame(\n",
    "                dense_tfidf,\n",
    "                columns=feature_names\n",
    "            )\n",
    "            \n",
    "            # Calculate average TF-IDF scores across all documents\n",
    "            avg_tfidf = pd.DataFrame({\n",
    "                'term': feature_names,\n",
    "                'avg_tfidf': doc_term_matrix.mean().values,\n",
    "                'docs_present': (doc_term_matrix > 0).sum().values\n",
    "            }).sort_values('avg_tfidf', ascending=False)\n",
    "            \n",
    "            # Save results\n",
    "            print(\"Saving analysis results...\")\n",
    "            \n",
    "            # Save average TF-IDF scores\n",
    "            avg_tfidf.to_csv(os.path.join(output_dir, 'average_tfidf_scores.csv'), index=False)\n",
    "            \n",
    "            # Save document-term matrix\n",
    "            doc_term_matrix.to_csv(os.path.join(output_dir, 'document_term_matrix.csv'))\n",
    "            \n",
    "            # Generate visualizations\n",
    "            print(\"Generating visualizations...\")\n",
    "            \n",
    "            # Plot top terms by average TF-IDF score\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            sns.barplot(data=avg_tfidf.head(20), x='avg_tfidf', y='term')\n",
    "            plt.title('Top 20 Terms by Average TF-IDF Score')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'top_terms_tfidf.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot term frequency distribution\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            sns.histplot(data=avg_tfidf, x='avg_tfidf', bins=50)\n",
    "            plt.title('Distribution of TF-IDF Scores')\n",
    "            plt.xlabel('TF-IDF Score')\n",
    "            plt.ylabel('Count')\n",
    "            plt.savefig(os.path.join(output_dir, 'tfidf_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Generate document similarity matrix\n",
    "            print(\"Calculating document similarity...\")\n",
    "            similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
    "            \n",
    "            # Plot document similarity heatmap\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(similarity_matrix[:50, :50], cmap='YlOrRd')  # Limited to first 50 documents for visibility\n",
    "            plt.title('Document Similarity Matrix (First 50 Documents)')\n",
    "            plt.savefig(os.path.join(output_dir, 'document_similarity.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Generate summary report\n",
    "            print(\"Generating summary report...\")\n",
    "            with open(os.path.join(output_dir, 'tfidf_analysis_summary.txt'), 'w') as f:\n",
    "                f.write(\"TF-IDF Analysis Summary\\n\")\n",
    "                f.write(\"=====================\\n\\n\")\n",
    "                \n",
    "                f.write(\"Document Statistics:\\n\")\n",
    "                f.write(f\"Total documents analyzed: {len(df)}\\n\")\n",
    "                f.write(f\"Average document length: {df['cleaned_text'].str.len().mean():.1f} characters\\n\")\n",
    "                f.write(f\"Unique terms analyzed: {len(feature_names)}\\n\\n\")\n",
    "                \n",
    "                f.write(\"Top 20 Most Important Terms (by average TF-IDF):\\n\")\n",
    "                f.write(avg_tfidf.head(20).to_string())\n",
    "                f.write(\"\\n\\nTF-IDF Score Statistics:\\n\")\n",
    "                f.write(f\"Mean TF-IDF score: {avg_tfidf['avg_tfidf'].mean():.4f}\\n\")\n",
    "                f.write(f\"Median TF-IDF score: {avg_tfidf['avg_tfidf'].median():.4f}\\n\")\n",
    "                f.write(f\"Max TF-IDF score: {avg_tfidf['avg_tfidf'].max():.4f}\\n\")\n",
    "                \n",
    "                # Find most unique terms (high TF-IDF, low document frequency)\n",
    "                unique_terms = avg_tfidf[avg_tfidf['docs_present'] <= len(df) * 0.1].head(20)\n",
    "                f.write(\"\\n\\nMost Unique Terms (high TF-IDF, present in <10% of documents):\\n\")\n",
    "                f.write(unique_terms.to_string())\n",
    "            \n",
    "            return doc_term_matrix, avg_tfidf, similarity_matrix\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during TF-IDF analysis: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = create_output_directory()\n",
    "    print(f\"\\nCreated output directory: {output_dir}\")\n",
    "    \n",
    "    # Read all CSV files in directory\n",
    "    csv_files = glob.glob('*.csv')\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found in current directory!\")\n",
    "        return\n",
    "    \n",
    "    # Read and combine all CSV files\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            print(f\"Read file: {file}\")\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {str(e)}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        print(\"No valid CSV files could be read!\")\n",
    "        return\n",
    "        \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Identify text column\n",
    "    possible_text_columns = ['text', 'description', 'comment', 'review', 'content', 'message']\n",
    "    text_columns = [col for col in combined_df.columns \n",
    "                   if any(text_name in col.lower() for text_name in possible_text_columns)]\n",
    "    \n",
    "    if not text_columns:\n",
    "        print(\"\\nNo obvious text columns found. Available columns are:\")\n",
    "        print(combined_df.columns.tolist())\n",
    "        text_column = input(\"\\nPlease enter the name of the text column to analyze: \")\n",
    "    else:\n",
    "        print(\"\\nFound potential text columns:\", text_columns)\n",
    "        if len(text_columns) == 1:\n",
    "            text_column = text_columns[0]\n",
    "        else:\n",
    "            text_column = input(\"\\nPlease enter the name of the text column to analyze: \")\n",
    "    \n",
    "    try:\n",
    "        # Perform analysis\n",
    "        doc_term_matrix, avg_tfidf, similarity_matrix = perform_tfidf_analysis(combined_df, text_column, output_dir)\n",
    "        \n",
    "        print(f\"\\nAnalysis completed! Results saved in: {output_dir}\")\n",
    "        print(\"\\nFiles generated:\")\n",
    "        print(\"1. average_tfidf_scores.csv - Average TF-IDF scores for each term\")\n",
    "        print(\"2. document_term_matrix.csv - Full document-term matrix\")\n",
    "        print(\"3. top_terms_tfidf.png - Bar plot of top terms by TF-IDF score\")\n",
    "        print(\"4. tfidf_distribution.png - Distribution of TF-IDF scores\")\n",
    "        print(\"5. document_similarity.png - Heatmap of document similarity\")\n",
    "        print(\"6. tfidf_analysis_summary.txt - Detailed analysis report\")\n",
    "        \n",
    "        return doc_term_matrix, avg_tfidf, similarity_matrix, output_dir\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during analysis: {str(e)}\")\n",
    "        return None, None, None, output_dir\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        results = main()\n",
    "        if results[0] is not None:\n",
    "            print(f\"\\nAnalysis completed successfully!\")\n",
    "        else:\n",
    "            print(\"\\nAnalysis completed with errors. Please check the output directory for partial results.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf345f1-e74d-40f5-a0e4-ac83b9d17500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
